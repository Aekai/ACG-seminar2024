{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWIMc0lIB4K1"
      },
      "source": [
        "# ACG Seminar Assignment -- Neural Volume Compression  \n",
        "\n",
        "Student: An≈æe Kristan  \n",
        "class: Advanced Computer Graphics  \n",
        "semester: Spring 2023/24  \n",
        "school: Faculty of Computer and Information Science, University of Ljubljana  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2M8dmjIDYC3u"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CEuX--IZ-ke"
      },
      "source": [
        "#### imports and setup:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VunWvEZAlI8R"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wK91ZzKZlWBs"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchsummary import summary\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from scipy.ndimage import gaussian_filter\n",
        "from os import path as ospath"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dLsUTnT3lXOc"
      },
      "outputs": [],
      "source": [
        "folder_path = \"todo\"\n",
        "files = {\n",
        "    \"tooth\": {\n",
        "        \"file_name\": \"tooth_103x94x161_1x1x1_uint8.raw\",\n",
        "        \"shape\": [103, 94, 161],\n",
        "        \"dtype\": np.uint8\n",
        "    }\n",
        "}\n",
        "\n",
        "# from https://www.geeksforgeeks.org/reading-binary-files-in-python/\n",
        "\n",
        "# Open the file in binary mode\n",
        "def readRawFile(file_path, datatype, shape, doReshape=True, divideBy=2**8, gaussianFilter=False, cutoffLow=(0,0), cutoffHigh=(255,255)):\n",
        "  array = np.fromfile(file_path, dtype=datatype).astype(np.float32) # read from file based on datatype\n",
        "  array[array<cutoffLow[0]] = cutoffLow[1] # set values below cutoff[0] to the value of cutoff[1]\n",
        "  array[array>cutoffHigh[0]] = cutoffHigh[1]\n",
        "  array = np.divide(array, divideBy) # divide input by specified (to get input into range 0-1)\n",
        "  if (doReshape):\n",
        "    array = np.reshape(array, list(reversed(shape))).transpose() # need to make sure reshape takes the same order as the file all x->all y->all z\n",
        "    if (gaussianFilter):\n",
        "      array = gaussian_filter(array, sigma=0.1, radius=3)\n",
        "  return array"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vss-WbLAYObS"
      },
      "source": [
        "#### Dataset class:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Pgh-GIJaC8f"
      },
      "source": [
        "Dataset class to help with machine learning; sample it like an array with an index (int) and returns a sample corresponding to that index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QIXSbByvl5e2"
      },
      "outputs": [],
      "source": [
        "class Dataset:\n",
        "  def __init__(self,samples,stride=[4,4,4],kernel_size=[8,8,8], returnSurroundingIndices=True):\n",
        "    self.stride = stride # [x,y,z]\n",
        "    self.kernel_size = kernel_size # single value treated as length of the side of a cube\n",
        "    self.returnSurroundingIndices = returnSurroundingIndices\n",
        "    self.orig_shape = samples.shape\n",
        "    self.samples = self.pad_to_divisible_and_kernel(samples)\n",
        "\n",
        "    self.unfold_dims = np.array([ # num samples in each dimension\n",
        "        int(np.ceil((self.orig_shape[0] - self.kernel_size[0])/self.stride[0])+1),\n",
        "        int(np.ceil((self.orig_shape[1] - self.kernel_size[1])/self.stride[1])+1),\n",
        "        int(np.ceil((self.orig_shape[2] - self.kernel_size[2])/self.stride[2])+1)\n",
        "    ])\n",
        "    self.length =  np.prod(self.unfold_dims) # total num samples\n",
        "\n",
        "  def pad_to_divisible_and_kernel(self, arr):\n",
        "    # pad the array to be divisible by kernel size\n",
        "    to_pad = np.remainder(arr.shape, self.kernel_size)\n",
        "    to_pad = [self.kernel_size[x] - to_pad[x] if to_pad[x] > 0 else 0 for x in range(3)]\n",
        "    arr_pd = np.pad(arr, ( (0, to_pad[0]), (0, to_pad[1]), (0, to_pad[2]) ), mode='constant', constant_values='0')\n",
        "\n",
        "    if self.returnSurroundingIndices:\n",
        "      # pad an extra kernel_size around array\n",
        "      extra_pad = [[self.kernel_size[x],self.kernel_size[x]] for x in range(3)]\n",
        "      arr_pd2 = torch.tensor(np.pad(arr_pd, extra_pad, mode='constant', constant_values=0))\n",
        "    else:\n",
        "      arr_pd2 = arr_pd\n",
        "    return torch.Tensor(arr_pd2)\n",
        "\n",
        "\n",
        "  def __getitem__(self,index):\n",
        "    arr_coord = np.multiply(np.unravel_index(index, self.unfold_dims), self.stride)\n",
        "    if self.returnSurroundingIndices:\n",
        "      arr_coord = np.add(arr_coord, self.kernel_size) # skip initial kernel_size of padding\n",
        "    ix = 0\n",
        "    if self.returnSurroundingIndices:\n",
        "      surrounding_indices = [-1, 0, 1]\n",
        "      ret_arr = torch.empty((27, *self.kernel_size))\n",
        "    else:\n",
        "      surrounding_indices = [0]\n",
        "      ret_arr = torch.empty((1, *self.kernel_size))\n",
        "    for z in surrounding_indices:\n",
        "      for y in surrounding_indices:\n",
        "        for x in surrounding_indices:\n",
        "          tmp_coords = np.add(arr_coord, np.multiply([x, y, z], self.kernel_size))\n",
        "          # print(ix, tmp_coords, z, y, x)\n",
        "          ret_arr[ix] = self.get_sub_matrix_from_coords(tmp_coords)\n",
        "          ix += 1\n",
        "    return ret_arr\n",
        "\n",
        "  def get_sub_matrix_from_coords(self, coords):\n",
        "    batch_start = coords\n",
        "    # print(batch_start)\n",
        "    batch_end = np.add(coords, self.kernel_size)\n",
        "    # print(batch_end)\n",
        "    ret_arr = self.samples[batch_start[0]:batch_end[0], batch_start[1]:batch_end[1], batch_start[2]:batch_end[2]]\n",
        "    # print(ret_arr.shape)\n",
        "    return ret_arr\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.length\n",
        "\n",
        "  def shuffle(self):\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gs7Bdey6Yco8"
      },
      "source": [
        "#### Fit function:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9AIamtnuAXw"
      },
      "source": [
        "Define the fit function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YGKp15QM1jBj"
      },
      "outputs": [],
      "source": [
        "from torch.optim import SGD, Adam\n",
        "from torch.nn import MSELoss, CrossEntropyLoss, L1Loss\n",
        "import datetime\n",
        "\n",
        "def fit(model, dset_train, num_epochs=1, train_batches=1, optimFunc=Adam, lossFunc=MSELoss, folder_path=folder_path, verbose=2):\n",
        "  # with help from https://pytorch.org/tutorials/beginner/introyt/trainingyt.html\n",
        "\n",
        "  curr_best_loss = np.Infinity\n",
        "  training_losses = []\n",
        "  save_path = ospath.join(folder_path, model.name)\n",
        "\n",
        "  optimizer = optimFunc(model.parameters(), lr=0.1)\n",
        "  loss = lossFunc()\n",
        "  model.train(True)\n",
        "  start_time = datetime.datetime.now()\n",
        "  if (verbose > 0):\n",
        "    print(f\"starting training for model '{model.name}' at: {datetime.datetime.now()}\")\n",
        "  for i in range(num_epochs):\n",
        "    t_losses = []\n",
        "\n",
        "    for j in range(train_batches):\n",
        "      x_train = dset_train[j]\n",
        "      p = model(x_train)\n",
        "      l = loss(p, x_train)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      l.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      t_losses.append(l.item())\n",
        "\n",
        "    curr_loss = np.mean(t_losses)\n",
        "    training_losses.append(curr_loss)\n",
        "\n",
        "    if curr_loss < curr_best_loss:\n",
        "      curr_best_loss = curr_loss\n",
        "      torch.save(model.state_dict(), save_path)\n",
        "    if (verbose > 1):\n",
        "      print(f\"epoch {i+1}/{num_epochs} at time {datetime.datetime.now().time()}; current loss: {curr_loss}\")\n",
        "    elif (verbose == 1):\n",
        "      print(f\"epoch {i+1}/{num_epochs: <{10}}\", end=\"\\r\")\n",
        "\n",
        "  end_time = datetime.datetime.now()\n",
        "  if (verbose > 0):\n",
        "    print(f\"end of training {num_epochs} epochs: {datetime.datetime.now()}; elapsed time: {end_time - start_time}\")\n",
        "  best_model = model\n",
        "  best_model.load_state_dict(torch.load(save_path))\n",
        "\n",
        "  return best_model, training_losses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DsIbRSAHhkOc"
      },
      "source": [
        "#### Linear models:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QO4fmrHPUX5r"
      },
      "source": [
        "##### AELinPass  \n",
        "Try a pass linear model to make sure it works"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8xP4Wsd4S3UQ"
      },
      "outputs": [],
      "source": [
        "class autoEncLinPass(nn.Module):\n",
        "\n",
        "  def __init__(self, name=\"linae1\", kernel_size=[8,8,8]):\n",
        "    super(autoEncLinPass, self).__init__()\n",
        "    sample_size = np.prod(kernel_size)\n",
        "    self.encoder = nn.Sequential(\n",
        "      nn.Flatten(),\n",
        "      )\n",
        "    self.decoder = nn.Sequential(\n",
        "      nn.Unflatten(1, [1, *kernel_size]),\n",
        "      )\n",
        "    self.name = name\n",
        "\n",
        "  def get_encoding(self, x):\n",
        "    return self.encoder(x)\n",
        "\n",
        "  def get_decoding(self, c):\n",
        "    return self.decoder(c)\n",
        "\n",
        "  def forward(self, x):\n",
        "    c = self.encoder(x)\n",
        "    y = self.decoder(c)\n",
        "    return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DHZ-U-IlURhc"
      },
      "outputs": [],
      "source": [
        "def trainAndOutputAutoEncPassModel(folder_path=folder_path, model=autoEncLinPass, kernel_size=[4,4,4], stride_size=[4,4,4], epochs=4, name=\"model1lin\", vol_name=\"tooth\", verbose=True):\n",
        "  # to train model\n",
        "  tooth_path=ospath.join(folder_path, files[vol_name][\"file_name\"])\n",
        "  tooth_array=readRawFile(tooth_path, files[vol_name][\"dtype\"], files[vol_name][\"shape\"], cutoffLow=(105,0))#, gaussianFilter=True)\n",
        "  model1toothDset = Dataset(tooth_array, stride=stride_size, kernel_size=kernel_size, returnSurroundingIndices=False)\n",
        "  model1 = model(name=name, kernel_size=kernel_size)\n",
        "\n",
        "  # to output encoded representation\n",
        "  if (verbose):\n",
        "    print(f\"starting encoding at: {datetime.datetime.now()}\")\n",
        "  toothEncDset = Dataset(tooth_array, stride=kernel_size, kernel_size=kernel_size, returnSurroundingIndices=False)\n",
        "  model1_encoded_reps = np.array([model1.encoder(toothEncDset[x]).detach().numpy().flatten() for x in range(toothEncDset.length)])\n",
        "  model1_encoded_reps.reshape(-1).tofile(ospath.join(folder_path, f\"{model1.name}.enc\"), format=\"%<f\")\n",
        "\n",
        "  # output reconstructed model\n",
        "  if (verbose):\n",
        "      print(f\"starting decoding at: {datetime.datetime.now()}\")\n",
        "  model1_reconstructed = np.empty(toothEncDset.samples.shape)\n",
        "  for ix, enc in enumerate(model1_encoded_reps):\n",
        "    tmp_dec = model1.decoder(torch.tensor(enc).reshape((1,-1))).detach().numpy()\n",
        "    ixst = np.multiply(np.unravel_index(ix, toothEncDset.unfold_dims), toothEncDset.stride)\n",
        "    ixed = np.add(ixst,toothEncDset.kernel_size)\n",
        "    # print(ix, ixst, ixed)\n",
        "    model1_reconstructed[ixst[0]:ixed[0], ixst[1]:ixed[1], ixst[2]:ixed[2]] = tmp_dec\n",
        "\n",
        "  model1_reconstructed = model1_reconstructed[0:toothEncDset.orig_shape[0], 0:toothEncDset.orig_shape[1], 0:toothEncDset.orig_shape[2]]\n",
        "  # model1_reconstructed = gaussian_filter(model1_reconstructed, sigma=1, radius=np.divide(kernel_size,2).astype(np.int32)) # filter output to get rid of some of the blockiness\n",
        "  mse = np.mean(np.power((model1_reconstructed*255 - tooth_array*255),2)) # from https://stackoverflow.com/a/18047482\n",
        "  print(f\"MSE reconstructed - input: {mse}\")\n",
        "  model1_reconstructed = (model1_reconstructed*255).astype(np.uint8).transpose().reshape(-1)\n",
        "  model1_reconstructed.tofile(ospath.join(folder_path, f\"{model1.name}.raw\"), format=\"%<hhf\")\n",
        "\n",
        "  return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VCklSMQ2TR-x"
      },
      "outputs": [],
      "source": [
        "model1_kernel = [8]*3\n",
        "model1_stride = [8]*3\n",
        "model1_name = \"linae1pass_8_8\"\n",
        "model1 = autoEncLinPass\n",
        "\n",
        "model1_epochs = 0 # 0 epochs just runs the encoder/decoder without training\n",
        "\n",
        "# trainAndOutputAutoEncPassModel(folder_path=folder_path, model=model1, kernel_size=model1_kernel, stride_size=model1_stride, epochs=model1_epochs, name=model1_name, vol_name=\"tooth\", verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_pxDvGoCYgl"
      },
      "source": [
        "##### AELin1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qaVQCC8GYfHV"
      },
      "outputs": [],
      "source": [
        "class autoEncLin1(nn.Module):\n",
        "\n",
        "  def __init__(self, name=\"linae1\", kernel_size=[8,8,8], enc_size=9):\n",
        "    super(autoEncLin1, self).__init__()\n",
        "    sample_size = np.prod(kernel_size)\n",
        "    self.encoder = nn.Sequential(\n",
        "      nn.Flatten(),\n",
        "      nn.Linear(in_features=sample_size, out_features=int(sample_size/2)),\n",
        "      nn.PReLU(), #use parametric relu for activation functions\n",
        "      nn.Linear(int(sample_size/2), int(sample_size/4)),\n",
        "      nn.PReLU(),\n",
        "      nn.Linear(int(sample_size/4), int(sample_size/8)),\n",
        "      nn.PReLU(),\n",
        "      nn.Linear(int(sample_size/8), enc_size),\n",
        "      nn.PReLU(),\n",
        "      )\n",
        "    self.decoder = nn.Sequential(\n",
        "      nn.Linear(enc_size, int(sample_size/8)),\n",
        "      nn.PReLU(),\n",
        "      nn.Linear(int(sample_size/8), int(sample_size/4)),\n",
        "      nn.PReLU(),\n",
        "      nn.Linear(int(sample_size/4), int(sample_size/2)),\n",
        "      nn.PReLU(),\n",
        "      nn.Linear(int(sample_size/2), int(sample_size)),\n",
        "      nn.Unflatten(1, kernel_size),\n",
        "      nn.Sigmoid(),\n",
        "      )\n",
        "    self.name = name\n",
        "\n",
        "  def get_encoding(self, x):\n",
        "    return self.encoder(x)\n",
        "\n",
        "  def get_decoding(self, c):\n",
        "    return self.decoder(c)\n",
        "\n",
        "  def forward(self, x):\n",
        "    c = self.encoder(x)\n",
        "    y = self.decoder(c)\n",
        "    return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CwVExSsnAVa2"
      },
      "outputs": [],
      "source": [
        "def trainAndOutputAutoEnc1Model(folder_path=folder_path, model=autoEncLin1, kernel_size=[4,4,4], stride_size=[4,4,4], enc_size=4, epochs=4, name=\"model1lin\", optim=SGD, loss=MSELoss, vol_name=\"tooth\", verbose=0, load_model_weights=False):\n",
        "  # to train model\n",
        "  tooth_path=ospath.join(folder_path, files[vol_name][\"file_name\"])\n",
        "  tooth_array=readRawFile(tooth_path, files[vol_name][\"dtype\"], files[vol_name][\"shape\"], cutoffLow=(100,0))#, gaussianFilter=True)\n",
        "  model1toothDset = Dataset(tooth_array, stride=stride_size, kernel_size=kernel_size, returnSurroundingIndices=False)\n",
        "  model1 = model(name=name, kernel_size=kernel_size, enc_size=enc_size)\n",
        "  if (load_model_weights):\n",
        "    save_path = ospath.join(folder_path, model1.name)\n",
        "    model1.load_state_dict(torch.load(save_path))\n",
        "  model1_best, model1_losses = fit(model1, model1toothDset, num_epochs=epochs, train_batches=model1toothDset.length-1, optimFunc=optim, lossFunc=loss, folder_path=folder_path, verbose=verbose)\n",
        "\n",
        "  # to output encoded representation\n",
        "  if (verbose > 0):\n",
        "    print(f\"starting encoding at: {datetime.datetime.now()}\")\n",
        "  toothEncDset = Dataset(tooth_array, stride=kernel_size, kernel_size=kernel_size, returnSurroundingIndices=False)\n",
        "  model1_encoded_reps = np.array([model1_best.encoder(toothEncDset[x]).detach().numpy().flatten().astype(np.float16) for x in range(toothEncDset.length)])\n",
        "  model1_encoded_reps.reshape(-1).tofile(ospath.join(folder_path, f\"{model1.name}.enc\"), format=\"%<hf\")\n",
        "\n",
        "  # output reconstructed model\n",
        "  if (verbose>0):\n",
        "      print(f\"starting decoding at: {datetime.datetime.now()}\")\n",
        "  model1_reconstructed = np.empty(toothEncDset.samples.shape)\n",
        "  for ix, enc in enumerate(model1_encoded_reps):\n",
        "    tmp_dec = model1_best.decoder(torch.tensor(enc.astype(np.float32)).reshape((1,-1))).detach().numpy()\n",
        "    ixst = np.multiply(np.unravel_index(ix, toothEncDset.unfold_dims), toothEncDset.stride)\n",
        "    ixed = np.add(ixst,toothEncDset.kernel_size)\n",
        "    # print(ix, ixst, ixed)\n",
        "    model1_reconstructed[ixst[0]:ixed[0], ixst[1]:ixed[1], ixst[2]:ixed[2]] = tmp_dec\n",
        "  if (verbose>0):\n",
        "      print(f\"finish decoding at: {datetime.datetime.now()}\")\n",
        "\n",
        "  model1_reconstructed = model1_reconstructed[0:toothEncDset.orig_shape[0], 0:toothEncDset.orig_shape[1], 0:toothEncDset.orig_shape[2]]\n",
        "  # model1_reconstructed = gaussian_filter(model1_reconstructed, sigma=1, radius=np.divide(kernel_size,2).astype(np.int32)) # filter output to get rid of some of the blockiness\n",
        "  mse = np.mean((np.multiply(model1_reconstructed,255) - np.multiply(tooth_array,255))**2) # from https://stackoverflow.com/a/18047482\n",
        "  print(f\"MSE: {mse}\")\n",
        "  model1_reconstructed = (model1_reconstructed*255).astype(np.uint8).transpose().reshape(-1)\n",
        "  model1_reconstructed.tofile(ospath.join(folder_path, f\"{model1.name}.raw\"), format=\"%<hhf\")\n",
        "\n",
        "  model1_decoder = model1_best\n",
        "  model1_decoder.encoder = None\n",
        "  torch.save(model1_decoder.state_dict(), ospath.join(folder_path, f\"{model1.name}.dec\"))\n",
        "\n",
        "  return model1_losses"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ae_lin1_load_weights = True\n",
        "ae_lin1_epochs = 0\n",
        "times_ran = 4 # x 50 epoch"
      ],
      "metadata": {
        "id": "9BnPNir_JOft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2t_minHgkciU",
        "outputId": "c9849fac-c3cb-411d-e0a6-f70369aea1e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "starting training for model 'linae1_8_8_1' at: 2024-06-07 11:29:28.326853\n",
            "epoch 1/49 at time 11:29:38.042561; current loss: 0.012241605034499095\n",
            "epoch 2/49 at time 11:29:50.937903; current loss: 0.012192569659117568\n",
            "epoch 3/49 at time 11:29:58.089216; current loss: 0.012141133235392064\n",
            "epoch 4/49 at time 11:30:06.619488; current loss: 0.012114875999025897\n",
            "epoch 5/49 at time 11:30:13.717756; current loss: 0.012074670573570248\n",
            "epoch 6/49 at time 11:30:21.922910; current loss: 0.012040309124047308\n",
            "epoch 7/49 at time 11:30:31.452817; current loss: 0.011994036796476955\n",
            "epoch 8/49 at time 11:30:38.604890; current loss: 0.01196519946526657\n",
            "epoch 9/49 at time 11:30:46.961539; current loss: 0.011953594385962413\n",
            "epoch 10/49 at time 11:30:55.316889; current loss: 0.011914332545025719\n",
            "epoch 11/49 at time 11:31:03.870879; current loss: 0.0119211454421794\n",
            "epoch 12/49 at time 11:31:11.087163; current loss: 0.011877693621400788\n",
            "epoch 13/49 at time 11:31:19.303560; current loss: 0.011853784167982015\n",
            "epoch 14/49 at time 11:31:27.045115; current loss: 0.011842657160948889\n",
            "epoch 15/49 at time 11:31:34.760031; current loss: 0.011831703752487132\n",
            "epoch 16/49 at time 11:31:43.223287; current loss: 0.011833317445282753\n",
            "epoch 17/49 at time 11:31:50.239469; current loss: 0.01182023563440515\n",
            "epoch 18/49 at time 11:31:58.877247; current loss: 0.01181207129979504\n",
            "epoch 19/49 at time 11:32:06.217914; current loss: 0.011797425789709451\n",
            "epoch 20/49 at time 11:32:14.809713; current loss: 0.011789998593145869\n",
            "epoch 21/49 at time 11:32:22.161479; current loss: 0.011796841699345468\n",
            "epoch 22/49 at time 11:32:30.404245; current loss: 0.011796820431051668\n",
            "epoch 23/49 at time 11:32:38.170473; current loss: 0.011801828745989555\n",
            "epoch 24/49 at time 11:32:46.002104; current loss: 0.01181297863190906\n",
            "epoch 25/49 at time 11:32:54.407013; current loss: 0.011838407486961876\n",
            "epoch 26/49 at time 11:33:01.407934; current loss: 0.01185777239138935\n",
            "epoch 27/49 at time 11:33:09.689986; current loss: 0.01187770961467468\n",
            "epoch 28/49 at time 11:33:16.611438; current loss: 0.01190134664608771\n",
            "epoch 29/49 at time 11:33:25.005622; current loss: 0.01193008883221361\n",
            "epoch 30/49 at time 11:33:32.045471; current loss: 0.011976549517088198\n",
            "epoch 31/49 at time 11:33:40.425359; current loss: 0.012102708287134296\n",
            "epoch 32/49 at time 11:33:47.566759; current loss: 0.012141318986387185\n",
            "epoch 33/49 at time 11:33:56.023550; current loss: 0.01215099324544779\n",
            "epoch 34/49 at time 11:34:03.751611; current loss: 0.012151144024363417\n",
            "epoch 35/49 at time 11:34:11.479152; current loss: 0.012160871140027209\n",
            "epoch 36/49 at time 11:34:19.377840; current loss: 0.01214627558895894\n",
            "epoch 37/49 at time 11:34:26.809077; current loss: 0.012088900114482066\n",
            "epoch 38/49 at time 11:34:35.342402; current loss: 0.012048415540902524\n",
            "epoch 39/49 at time 11:34:42.393650; current loss: 0.011997135663337544\n",
            "epoch 40/49 at time 11:34:51.367277; current loss: 0.011945633452255642\n",
            "epoch 41/49 at time 11:34:59.059111; current loss: 0.011822149012592154\n",
            "epoch 42/49 at time 11:35:07.511913; current loss: 0.011744917453214395\n",
            "epoch 43/49 at time 11:35:15.199453; current loss: 0.011665812308392873\n",
            "epoch 44/49 at time 11:35:23.077722; current loss: 0.011615232667331336\n",
            "epoch 45/49 at time 11:35:31.129960; current loss: 0.01159311855288609\n",
            "epoch 46/49 at time 11:35:38.690651; current loss: 0.011559883421113495\n",
            "epoch 47/49 at time 11:35:47.055752; current loss: 0.01161024239134882\n",
            "epoch 48/49 at time 11:35:54.149816; current loss: 0.01155946238760921\n",
            "epoch 49/49 at time 11:36:02.594663; current loss: 0.011667592980222881\n",
            "end of training 49 epochs: 2024-06-07 11:36:02.594821; elapsed time: 0:06:34.267972\n",
            "starting encoding at: 2024-06-07 11:36:02.607281\n",
            "starting decoding at: 2024-06-07 11:36:03.757325\n",
            "finish decoding at: 2024-06-07 11:36:04.800733\n",
            "MSE: 1757.6536980638905\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    49.000000\n",
              "mean      0.011906\n",
              "std       0.000178\n",
              "min       0.011559\n",
              "25%       0.011802\n",
              "50%       0.011878\n",
              "75%       0.012048\n",
              "max       0.012242\n",
              "dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 140
        }
      ],
      "source": [
        "model1_kernel = [8]*3\n",
        "model1_stride = [8]*3\n",
        "model1_enc_size = 1\n",
        "model1_name = f\"linae1_8_8_{int(model1_enc_size)}\"\n",
        "model1 = autoEncLin1\n",
        "\n",
        "model1_epochs = ae_lin1_epochs # 0 epochs just runs the encoder/decoder without training\n",
        "model1_optim = SGD  #Adam#SGD\n",
        "model1_loss = MSELoss   #CrossEntropyLoss#MSELoss\n",
        "model1_load_weights = ae_lin1_load_weights # if changing anything above this line (especially kernel,stride,enc_size,name), set load_model_weights to False\n",
        "\n",
        "model1_losses = trainAndOutputAutoEnc1Model(folder_path, model1, model1_kernel, model1_stride, model1_enc_size, model1_epochs, model1_name, model1_optim, model1_loss, vol_name=\"tooth\", verbose=2, load_model_weights=model1_load_weights)\n",
        "pd.Series(model1_losses).describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7b71c46-9fa7-449d-8605-3860f73cc859",
        "id": "42nIzmifI4Zy"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "starting training for model 'linae1_8_8_3' at: 2024-06-07 11:36:04.913107\n",
            "epoch 1/49 at time 11:36:12.524960; current loss: 0.003937267553219117\n",
            "epoch 2/49 at time 11:36:20.630453; current loss: 0.003915317802647959\n",
            "epoch 3/49 at time 11:36:28.964589; current loss: 0.003889330925812776\n",
            "epoch 4/49 at time 11:36:36.188570; current loss: 0.0038662638063148478\n",
            "epoch 5/49 at time 11:36:44.657457; current loss: 0.0038445766860840686\n",
            "epoch 6/49 at time 11:36:51.627873; current loss: 0.00382180291097437\n",
            "epoch 7/49 at time 11:36:59.860814; current loss: 0.00380405185302841\n",
            "epoch 8/49 at time 11:37:06.856378; current loss: 0.003772916415880879\n",
            "epoch 9/49 at time 11:37:15.504108; current loss: 0.003753191327936213\n",
            "epoch 10/49 at time 11:37:22.869048; current loss: 0.0037358023342690346\n",
            "epoch 11/49 at time 11:37:30.927041; current loss: 0.0037157232889023038\n",
            "epoch 12/49 at time 11:37:38.759476; current loss: 0.0036946721630491826\n",
            "epoch 13/49 at time 11:37:46.592717; current loss: 0.0036718134393459835\n",
            "epoch 14/49 at time 11:37:55.181468; current loss: 0.003648079082719929\n",
            "epoch 15/49 at time 11:38:02.317220; current loss: 0.00362297996350546\n",
            "epoch 16/49 at time 11:38:10.780577; current loss: 0.0036064128497016188\n",
            "epoch 17/49 at time 11:38:18.162205; current loss: 0.0035733814472899503\n",
            "epoch 18/49 at time 11:38:26.591942; current loss: 0.0035505445936829722\n",
            "epoch 19/49 at time 11:38:33.855577; current loss: 0.0035314848809398533\n",
            "epoch 20/49 at time 11:38:42.121060; current loss: 0.0035164555096970553\n",
            "epoch 21/49 at time 11:38:49.922025; current loss: 0.0034961809198886164\n",
            "epoch 22/49 at time 11:38:58.875119; current loss: 0.003464831487548689\n",
            "epoch 23/49 at time 11:39:07.309133; current loss: 0.0034345286571485253\n",
            "epoch 24/49 at time 11:39:14.348712; current loss: 0.0034206454448083966\n",
            "epoch 25/49 at time 11:39:22.844461; current loss: 0.0033789123349512178\n",
            "epoch 26/49 at time 11:39:29.827666; current loss: 0.00335903890067644\n",
            "epoch 27/49 at time 11:39:38.239824; current loss: 0.0033303838160780547\n",
            "epoch 28/49 at time 11:39:45.526147; current loss: 0.003289331264282747\n",
            "epoch 29/49 at time 11:39:53.895453; current loss: 0.003264757138682235\n",
            "epoch 30/49 at time 11:40:01.678102; current loss: 0.003220261481493272\n",
            "epoch 31/49 at time 11:40:09.341639; current loss: 0.0031968181080105577\n",
            "epoch 32/49 at time 11:40:17.639749; current loss: 0.003203673428338117\n",
            "epoch 33/49 at time 11:40:24.786659; current loss: 0.0031610187166467647\n",
            "epoch 34/49 at time 11:40:33.125448; current loss: 0.0031396670214133317\n",
            "epoch 35/49 at time 11:40:40.115987; current loss: 0.0030734841262378913\n",
            "epoch 36/49 at time 11:40:48.559901; current loss: 0.003038109220155065\n",
            "epoch 37/49 at time 11:40:55.677671; current loss: 0.0030142974920268632\n",
            "epoch 38/49 at time 11:41:04.033307; current loss: 0.002979952531609212\n",
            "epoch 39/49 at time 11:41:11.820946; current loss: 0.002962894357528268\n",
            "epoch 40/49 at time 11:41:19.586408; current loss: 0.0029462122497243754\n",
            "epoch 41/49 at time 11:41:27.538575; current loss: 0.0029305860578166133\n",
            "epoch 42/49 at time 11:41:34.930294; current loss: 0.0029061862971794775\n",
            "epoch 43/49 at time 11:41:43.370472; current loss: 0.002869511782155058\n",
            "epoch 44/49 at time 11:41:50.386366; current loss: 0.0028636616641427174\n",
            "epoch 45/49 at time 11:41:58.854564; current loss: 0.0028315713985239545\n",
            "epoch 46/49 at time 11:42:05.873352; current loss: 0.0028203067610837965\n",
            "epoch 47/49 at time 11:42:14.165103; current loss: 0.0027854160601922563\n",
            "epoch 48/49 at time 11:42:21.325412; current loss: 0.002780617539611238\n",
            "epoch 49/49 at time 11:42:29.715410; current loss: 0.0027665702000126526\n",
            "end of training 49 epochs: 2024-06-07 11:42:29.716626; elapsed time: 0:06:24.803517\n",
            "starting encoding at: 2024-06-07 11:42:29.727852\n",
            "starting decoding at: 2024-06-07 11:42:30.882582\n",
            "finish decoding at: 2024-06-07 11:42:31.909340\n",
            "MSE: 477.66489662526766\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    49.000000\n",
              "mean      0.003355\n",
              "std       0.000366\n",
              "min       0.002767\n",
              "25%       0.003014\n",
              "50%       0.003379\n",
              "75%       0.003672\n",
              "max       0.003937\n",
              "dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 141
        }
      ],
      "source": [
        "model1_kernel = [8]*3\n",
        "model1_stride = [8]*3\n",
        "model1_enc_size = 3\n",
        "model1_name = f\"linae1_8_8_{int(model1_enc_size)}\"\n",
        "model1 = autoEncLin1\n",
        "\n",
        "model1_epochs = ae_lin1_epochs # 0 epochs just runs the encoder/decoder without training\n",
        "model1_optim = SGD  #Adam#SGD\n",
        "model1_loss = MSELoss   #CrossEntropyLoss#MSELoss\n",
        "model1_load_weights = ae_lin1_load_weights # if changing anything above this line (especially kernel,stride,enc_size,name), set load_model_weights to False\n",
        "\n",
        "model1_losses = trainAndOutputAutoEnc1Model(folder_path, model1, model1_kernel, model1_stride, model1_enc_size, model1_epochs, model1_name, model1_optim, model1_loss, vol_name=\"tooth\", verbose=2, load_model_weights=model1_load_weights)\n",
        "pd.Series(model1_losses).describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2d37e24-f2c0-4703-c00f-ef8b62752674",
        "id": "WBkq-t4DI6KJ"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "starting training for model 'linae1_8_8_5' at: 2024-06-07 11:42:32.043044\n",
            "epoch 1/49 at time 11:42:40.726020; current loss: 0.0016859530235841223\n",
            "epoch 2/49 at time 11:42:47.720541; current loss: 0.0016810546274816525\n",
            "epoch 3/49 at time 11:42:58.111158; current loss: 0.001676681993241361\n",
            "epoch 4/49 at time 11:43:05.173913; current loss: 0.0016661041352288865\n",
            "epoch 5/49 at time 11:43:13.683895; current loss: 0.0016603869294785024\n",
            "epoch 6/49 at time 11:43:20.890925; current loss: 0.0016564296459121073\n",
            "epoch 7/49 at time 11:43:29.444276; current loss: 0.0016437510402228786\n",
            "epoch 8/49 at time 11:43:36.694489; current loss: 0.0016388240233525023\n",
            "epoch 9/49 at time 11:43:45.154498; current loss: 0.0016343912878222673\n",
            "epoch 10/49 at time 11:43:52.844232; current loss: 0.0016279164677596952\n",
            "epoch 11/49 at time 11:44:00.597232; current loss: 0.0016213531655147777\n",
            "epoch 12/49 at time 11:44:08.729932; current loss: 0.001614038339990806\n",
            "epoch 13/49 at time 11:44:15.906550; current loss: 0.0016113878883923546\n",
            "epoch 14/49 at time 11:44:24.197866; current loss: 0.0016036962329658204\n",
            "epoch 15/49 at time 11:44:31.251580; current loss: 0.0016016823506961356\n",
            "epoch 16/49 at time 11:44:39.657847; current loss: 0.001599803134864298\n",
            "epoch 17/49 at time 11:44:46.662242; current loss: 0.001590719730321726\n",
            "epoch 18/49 at time 11:44:55.093987; current loss: 0.0015863704002161889\n",
            "epoch 19/49 at time 11:45:02.262436; current loss: 0.001585106898515526\n",
            "epoch 20/49 at time 11:45:10.565625; current loss: 0.0015829957160623967\n",
            "epoch 21/49 at time 11:45:18.590554; current loss: 0.001573425366722246\n",
            "epoch 22/49 at time 11:45:26.444390; current loss: 0.0015739529748942157\n",
            "epoch 23/49 at time 11:45:35.078035; current loss: 0.0015683570978467982\n",
            "epoch 24/49 at time 11:45:42.201447; current loss: 0.001564939786353686\n",
            "epoch 25/49 at time 11:45:50.533512; current loss: 0.00155978460523598\n",
            "epoch 26/49 at time 11:45:57.579876; current loss: 0.0015562243626124797\n",
            "epoch 27/49 at time 11:46:06.103059; current loss: 0.0015507692640972545\n",
            "epoch 28/49 at time 11:46:13.294282; current loss: 0.0015502978555936506\n",
            "epoch 29/49 at time 11:46:21.753024; current loss: 0.0015447213355180287\n",
            "epoch 30/49 at time 11:46:29.710619; current loss: 0.001539720231468834\n",
            "epoch 31/49 at time 11:46:37.514728; current loss: 0.0015380455362090518\n",
            "epoch 32/49 at time 11:46:46.103824; current loss: 0.0015385178977095132\n",
            "epoch 33/49 at time 11:46:54.696896; current loss: 0.0015351527262793426\n",
            "epoch 34/49 at time 11:47:03.240504; current loss: 0.0015326453752301254\n",
            "epoch 35/49 at time 11:47:10.243912; current loss: 0.0015303779419618296\n",
            "epoch 36/49 at time 11:47:18.610210; current loss: 0.0015222879475207608\n",
            "epoch 37/49 at time 11:47:26.167191; current loss: 0.0015170175681176882\n",
            "epoch 38/49 at time 11:47:34.071653; current loss: 0.0015224593554825452\n",
            "epoch 39/49 at time 11:47:41.898491; current loss: 0.001523381404835348\n",
            "epoch 40/49 at time 11:47:49.421528; current loss: 0.0015273309009836394\n",
            "epoch 41/49 at time 11:47:57.782489; current loss: 0.0015434725224581997\n",
            "epoch 42/49 at time 11:48:04.938441; current loss: 0.0015398443816871208\n",
            "epoch 43/49 at time 11:48:13.409211; current loss: 0.0015157938296317015\n",
            "epoch 44/49 at time 11:48:20.579978; current loss: 0.0014999295317817496\n",
            "epoch 45/49 at time 11:48:29.187571; current loss: 0.0014969708873189152\n",
            "epoch 46/49 at time 11:48:36.261029; current loss: 0.0014987336938968219\n",
            "epoch 47/49 at time 11:48:44.532491; current loss: 0.0014884996491888525\n",
            "epoch 48/49 at time 11:48:51.961328; current loss: 0.0014937665081204584\n",
            "epoch 49/49 at time 11:48:59.949384; current loss: 0.0014857117884670052\n",
            "end of training 49 epochs: 2024-06-07 11:48:59.951237; elapsed time: 0:06:27.908194\n",
            "starting encoding at: 2024-06-07 11:48:59.960668\n",
            "starting decoding at: 2024-06-07 11:49:01.102845\n",
            "finish decoding at: 2024-06-07 11:49:02.131538\n",
            "MSE: 137.40241579261757\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    49.000000\n",
              "mean      0.001571\n",
              "std       0.000056\n",
              "min       0.001486\n",
              "25%       0.001530\n",
              "50%       0.001560\n",
              "75%       0.001611\n",
              "max       0.001686\n",
              "dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 142
        }
      ],
      "source": [
        "model1_kernel = [8]*3\n",
        "model1_stride = [8]*3\n",
        "model1_enc_size = 5\n",
        "model1_name = f\"linae1_8_8_{int(model1_enc_size)}\"\n",
        "model1 = autoEncLin1\n",
        "\n",
        "model1_epochs = ae_lin1_epochs # 0 epochs just runs the encoder/decoder without training\n",
        "model1_optim = SGD  #Adam#SGD\n",
        "model1_loss = MSELoss   #CrossEntropyLoss#MSELoss\n",
        "model1_load_weights = ae_lin1_load_weights # if changing anything above this line (especially kernel,stride,enc_size,name), set load_model_weights to False\n",
        "\n",
        "model1_losses = trainAndOutputAutoEnc1Model(folder_path, model1, model1_kernel, model1_stride, model1_enc_size, model1_epochs, model1_name, model1_optim, model1_loss, vol_name=\"tooth\", verbose=2, load_model_weights=model1_load_weights)\n",
        "pd.Series(model1_losses).describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e58bab8b-2bc6-4b1e-a4da-4e25a01dd255",
        "id": "dsFCRiVAJD4u"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "starting training for model 'linae1_8_8_7' at: 2024-06-07 11:49:02.235095\n",
            "epoch 1/49 at time 11:49:10.675616; current loss: 0.0018950033703691684\n",
            "epoch 2/49 at time 11:49:17.771508; current loss: 0.001887446522323305\n",
            "epoch 3/49 at time 11:49:26.333236; current loss: 0.0018792999016516085\n",
            "epoch 4/49 at time 11:49:33.740400; current loss: 0.0018719941295304119\n",
            "epoch 5/49 at time 11:49:42.061426; current loss: 0.0018638042131196024\n",
            "epoch 6/49 at time 11:49:49.700157; current loss: 0.0018556899880766873\n",
            "epoch 7/49 at time 11:49:57.470675; current loss: 0.0018487900015389913\n",
            "epoch 8/49 at time 11:50:05.889737; current loss: 0.001841413877159157\n",
            "epoch 9/49 at time 11:50:13.286567; current loss: 0.0018338862416870678\n",
            "epoch 10/49 at time 11:50:21.862156; current loss: 0.0018267869864330142\n",
            "epoch 11/49 at time 11:50:29.047366; current loss: 0.0018197948661752834\n",
            "epoch 12/49 at time 11:50:37.534497; current loss: 0.0018129960358311034\n",
            "epoch 13/49 at time 11:50:44.549640; current loss: 0.0018054555645449694\n",
            "epoch 14/49 at time 11:50:54.899759; current loss: 0.0017993342828193962\n",
            "epoch 15/49 at time 11:51:01.979608; current loss: 0.001792228228990042\n",
            "epoch 16/49 at time 11:51:10.190427; current loss: 0.0017859944935707279\n",
            "epoch 17/49 at time 11:51:17.729152; current loss: 0.001779318945466995\n",
            "epoch 18/49 at time 11:51:25.701898; current loss: 0.0017728411027978803\n",
            "epoch 19/49 at time 11:51:33.826434; current loss: 0.00176652738143723\n",
            "epoch 20/49 at time 11:51:41.043862; current loss: 0.0017608901737452374\n",
            "epoch 21/49 at time 11:51:49.535025; current loss: 0.0017548549275356855\n",
            "epoch 22/49 at time 11:51:56.621233; current loss: 0.0017487732403513375\n",
            "epoch 23/49 at time 11:52:05.097992; current loss: 0.001743630926122398\n",
            "epoch 24/49 at time 11:52:12.036469; current loss: 0.0017377329954521834\n",
            "epoch 25/49 at time 11:52:20.399942; current loss: 0.0017311908600721774\n",
            "epoch 26/49 at time 11:52:27.391672; current loss: 0.0017263723880916989\n",
            "epoch 27/49 at time 11:52:35.751985; current loss: 0.001720934920413441\n",
            "epoch 28/49 at time 11:52:43.268199; current loss: 0.0017152788563410864\n",
            "epoch 29/49 at time 11:52:51.087803; current loss: 0.0017098255894973744\n",
            "epoch 30/49 at time 11:52:59.039216; current loss: 0.0017048268268791194\n",
            "epoch 31/49 at time 11:53:06.501846; current loss: 0.001699402385523307\n",
            "epoch 32/49 at time 11:53:14.945354; current loss: 0.0016946710426216196\n",
            "epoch 33/49 at time 11:53:21.997712; current loss: 0.0016897182164050976\n",
            "epoch 34/49 at time 11:53:30.479611; current loss: 0.0016850837799262178\n",
            "epoch 35/49 at time 11:53:37.641862; current loss: 0.001680810051507928\n",
            "epoch 36/49 at time 11:53:46.109486; current loss: 0.001675572278490167\n",
            "epoch 37/49 at time 11:53:53.297860; current loss: 0.0016707591763747067\n",
            "epoch 38/49 at time 11:54:01.743857; current loss: 0.0016661282280699392\n",
            "epoch 39/49 at time 11:54:09.297307; current loss: 0.0016620630517343129\n",
            "epoch 40/49 at time 11:54:17.323896; current loss: 0.0016569586390206663\n",
            "epoch 41/49 at time 11:54:25.338673; current loss: 0.0016532060317655047\n",
            "epoch 42/49 at time 11:54:33.049293; current loss: 0.0016478676266210087\n",
            "epoch 43/49 at time 11:54:41.754528; current loss: 0.0016439351126108573\n",
            "epoch 44/49 at time 11:54:49.051346; current loss: 0.001638286504617967\n",
            "epoch 45/49 at time 11:54:58.592058; current loss: 0.0016355593935767994\n",
            "epoch 46/49 at time 11:55:06.289708; current loss: 0.0016312975116144456\n",
            "epoch 47/49 at time 11:55:14.400225; current loss: 0.0016269640121675517\n",
            "epoch 48/49 at time 11:55:21.982487; current loss: 0.0016235873897813302\n",
            "epoch 49/49 at time 11:55:29.835259; current loss: 0.001619598956402927\n",
            "end of training 49 epochs: 2024-06-07 11:55:29.835484; elapsed time: 0:06:27.600391\n",
            "starting encoding at: 2024-06-07 11:55:29.845270\n",
            "starting decoding at: 2024-06-07 11:55:30.917415\n",
            "finish decoding at: 2024-06-07 11:55:31.953510\n",
            "MSE: 140.50627157720544\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    49.000000\n",
              "mean      0.001741\n",
              "std       0.000082\n",
              "min       0.001620\n",
              "25%       0.001671\n",
              "50%       0.001731\n",
              "75%       0.001805\n",
              "max       0.001895\n",
              "dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 143
        }
      ],
      "source": [
        "model1_kernel = [8]*3\n",
        "model1_stride = [8]*3\n",
        "model1_enc_size = 7\n",
        "model1_name = f\"linae1_8_8_{int(model1_enc_size)}\"\n",
        "model1 = autoEncLin1\n",
        "\n",
        "model1_epochs = ae_lin1_epochs # 0 epochs just runs the encoder/decoder without training\n",
        "model1_optim = SGD  #Adam#SGD\n",
        "model1_loss = MSELoss   #CrossEntropyLoss#MSELoss\n",
        "model1_load_weights = ae_lin1_load_weights # if changing anything above this line (especially kernel,stride,enc_size,name), set load_model_weights to False\n",
        "\n",
        "model1_losses = trainAndOutputAutoEnc1Model(folder_path, model1, model1_kernel, model1_stride, model1_enc_size, model1_epochs, model1_name, model1_optim, model1_loss, vol_name=\"tooth\", verbose=2, load_model_weights=model1_load_weights)\n",
        "pd.Series(model1_losses).describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPM284W_loUw"
      },
      "source": [
        "##### AELin2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ZQiIbCNe6Ks"
      },
      "outputs": [],
      "source": [
        "class autoEncLin2(nn.Module):\n",
        "\n",
        "  def __init__(self, name=\"linae2\", kernel_size=[8,8,8], enc_size=9):\n",
        "    super(autoEncLin2, self).__init__()\n",
        "    sample_size = np.prod(kernel_size)\n",
        "    self.encoder = nn.Sequential(\n",
        "      nn.Flatten(),\n",
        "      nn.Linear(in_features=sample_size, out_features=int(sample_size/8)),\n",
        "      nn.PReLU(), #use parametric relu for activation functions\n",
        "      nn.Linear(int(sample_size/8), int(sample_size/8)),\n",
        "      nn.PReLU(),\n",
        "      nn.Linear(int(sample_size/8), int(sample_size/12)),\n",
        "      nn.PReLU(),\n",
        "      nn.Linear(int(sample_size/12), enc_size),\n",
        "      nn.PReLU(),\n",
        "      )\n",
        "    self.decoder = nn.Sequential(\n",
        "      nn.Linear(enc_size, int(sample_size/12)),\n",
        "      nn.PReLU(),\n",
        "      nn.Linear(int(sample_size/12), int(sample_size/8)),\n",
        "      nn.PReLU(),\n",
        "      nn.Linear(int(sample_size/8), int(sample_size/8)),\n",
        "      nn.PReLU(),\n",
        "      nn.Linear(int(sample_size/8), int(sample_size)),\n",
        "      nn.Unflatten(1, kernel_size),\n",
        "      nn.Sigmoid(),\n",
        "      )\n",
        "    self.name = name\n",
        "\n",
        "  def get_encoding(self, x):\n",
        "    return self.encoder(x)\n",
        "\n",
        "  def get_decoding(self, c):\n",
        "    return self.decoder(c)\n",
        "\n",
        "  def forward(self, x):\n",
        "    c = self.encoder(x)\n",
        "    y = self.decoder(c)\n",
        "    return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DcY0OG-TeTeE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93183d94-6c83-430b-dd8d-d2a9f5048857"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "starting training for model 'linae2_8_8_5' at: 2024-06-07 12:42:25.716701\n",
            "epoch 1/50 at time 12:42:32.508813; current loss: 0.008250118583926326\n",
            "epoch 2/50 at time 12:42:38.802978; current loss: 0.00822546267544442\n",
            "epoch 3/50 at time 12:42:44.612602; current loss: 0.008197530948051309\n",
            "epoch 4/50 at time 12:42:52.423414; current loss: 0.008162431728912196\n",
            "epoch 5/50 at time 12:42:58.813913; current loss: 0.00812143541303491\n",
            "epoch 6/50 at time 12:43:05.810489; current loss: 0.008071156316751819\n",
            "epoch 7/50 at time 12:43:11.565482; current loss: 0.008031430634942384\n",
            "epoch 8/50 at time 12:43:18.697559; current loss: 0.007979225988696418\n",
            "epoch 9/50 at time 12:43:24.439635; current loss: 0.007928454264928065\n",
            "epoch 10/50 at time 12:43:31.843367; current loss: 0.00787019931321347\n",
            "epoch 11/50 at time 12:43:37.599003; current loss: 0.007797171473719916\n",
            "epoch 12/50 at time 12:43:44.065187; current loss: 0.007725864063646955\n",
            "epoch 13/50 at time 12:43:50.477234; current loss: 0.007662549344243725\n",
            "epoch 14/50 at time 12:43:56.394051; current loss: 0.007587332196710264\n",
            "epoch 15/50 at time 12:44:03.541833; current loss: 0.0075345618048295055\n",
            "epoch 16/50 at time 12:44:09.202470; current loss: 0.007439977149361326\n",
            "epoch 17/50 at time 12:44:16.296853; current loss: 0.007382175154058828\n",
            "epoch 18/50 at time 12:44:22.115086; current loss: 0.0073627726273370435\n",
            "epoch 19/50 at time 12:44:29.284737; current loss: 0.007273521226020612\n",
            "epoch 20/50 at time 12:44:35.024969; current loss: 0.007222368538095455\n",
            "epoch 21/50 at time 12:44:41.447319; current loss: 0.007135038404998191\n",
            "epoch 22/50 at time 12:44:47.791745; current loss: 0.007055176482701508\n",
            "epoch 23/50 at time 12:44:53.628680; current loss: 0.006992351229138211\n",
            "epoch 24/50 at time 12:45:00.721101; current loss: 0.006913214949218509\n",
            "epoch 25/50 at time 12:45:06.484511; current loss: 0.006811072726402758\n",
            "epoch 26/50 at time 12:45:13.789589; current loss: 0.006666259300438733\n",
            "epoch 27/50 at time 12:45:19.563343; current loss: 0.006339185159338251\n",
            "epoch 28/50 at time 12:45:26.767059; current loss: 0.0056022634806852534\n",
            "epoch 29/50 at time 12:45:32.520957; current loss: 0.005249831193225245\n",
            "epoch 30/50 at time 12:45:39.157848; current loss: 0.005104522880757649\n",
            "epoch 31/50 at time 12:45:45.514171; current loss: 0.005019714239608692\n",
            "epoch 32/50 at time 12:45:51.548129; current loss: 0.0049564910554103095\n",
            "epoch 33/50 at time 12:45:58.419129; current loss: 0.00490874066846458\n",
            "epoch 34/50 at time 12:46:04.216407; current loss: 0.0048745787475635355\n",
            "epoch 35/50 at time 12:46:11.307093; current loss: 0.00484025218750621\n",
            "epoch 36/50 at time 12:46:17.296433; current loss: 0.004812562818375324\n",
            "epoch 37/50 at time 12:46:24.588859; current loss: 0.004787506006092955\n",
            "epoch 38/50 at time 12:46:30.440080; current loss: 0.0047663046152336686\n",
            "epoch 39/50 at time 12:46:37.468863; current loss: 0.004745766086963429\n",
            "epoch 40/50 at time 12:46:43.155737; current loss: 0.004726024281652208\n",
            "epoch 41/50 at time 12:46:49.436418; current loss: 0.004706865544570044\n",
            "epoch 42/50 at time 12:46:57.548252; current loss: 0.004688481728242633\n",
            "epoch 43/50 at time 12:47:03.529838; current loss: 0.0046720502870556705\n",
            "epoch 44/50 at time 12:47:10.233813; current loss: 0.004659180542217405\n",
            "epoch 45/50 at time 12:47:15.841722; current loss: 0.004643875631223664\n",
            "epoch 46/50 at time 12:47:22.926532; current loss: 0.004627752456001712\n",
            "epoch 47/50 at time 12:47:28.751745; current loss: 0.0046134012424301945\n",
            "epoch 48/50 at time 12:47:35.873041; current loss: 0.004597524709657341\n",
            "epoch 49/50 at time 12:47:41.610355; current loss: 0.004582797901364353\n",
            "epoch 50/50 at time 12:47:48.293857; current loss: 0.004566241999909904\n",
            "end of training 50 epochs: 2024-06-07 12:47:48.294038; elapsed time: 0:05:22.577339\n",
            "starting encoding at: 2024-06-07 12:47:48.302242\n",
            "starting decoding at: 2024-06-07 12:47:49.619963\n",
            "finish decoding at: 2024-06-07 12:47:50.512156\n",
            "MSE: 341.75756985646404\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    50.000000\n",
              "mean      0.006290\n",
              "std       0.001437\n",
              "min       0.004566\n",
              "25%       0.004772\n",
              "50%       0.006739\n",
              "75%       0.007644\n",
              "max       0.008250\n",
              "dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 154
        }
      ],
      "source": [
        "model1_kernel = [8]*3\n",
        "model1_stride = [8]*3\n",
        "model1_enc_size = 5\n",
        "model1_name = f\"linae2_8_8_{model1_enc_size}\"\n",
        "model1 = autoEncLin2\n",
        "\n",
        "model1_epochs = 0 # 0 epochs just runs the encoder/decoder without training\n",
        "model1_optim = SGD  #Adam#SGD\n",
        "model1_loss = MSELoss   #CrossEntropyLoss#MSELoss\n",
        "model1_load_weights = True # if changing anything above this line (especially kernel,stride,enc_size,name), set load_model_weights to False\n",
        "\n",
        "model1_losses = trainAndOutputAutoEnc1Model(folder_path, model1, model1_kernel, model1_stride, model1_enc_size, model1_epochs, model1_name, model1_optim, model1_loss, vol_name=\"tooth\", verbose=2, load_model_weights=model1_load_weights)\n",
        "pd.Series(model1_losses).describe()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model1_kernel = [8]*3\n",
        "model1_stride = [8]*3\n",
        "model1_enc_size = 7\n",
        "model1_name = f\"linae2_8_8_{model1_enc_size}\"\n",
        "model1 = autoEncLin2\n",
        "\n",
        "model1_epochs = 0 # 0 epochs just runs the encoder/decoder without training\n",
        "model1_optim = SGD  #Adam#SGD\n",
        "model1_loss = MSELoss   #CrossEntropyLoss#MSELoss\n",
        "model1_load_weights = True # if changing anything above this line (especially kernel,stride,enc_size,name), set load_model_weights to False\n",
        "\n",
        "model1_losses = trainAndOutputAutoEnc1Model(folder_path, model1, model1_kernel, model1_stride, model1_enc_size, model1_epochs, model1_name, model1_optim, model1_loss, vol_name=\"tooth\", verbose=2, load_model_weights=model1_load_weights)\n",
        "pd.Series(model1_losses).describe()"
      ],
      "metadata": {
        "id": "yGE9XgbDQ70I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ede0dade-7650-4865-fe2d-758a28835f02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "starting training for model 'linae2_8_8_7' at: 2024-06-07 12:56:32.933052\n",
            "epoch 1/50 at time 12:56:39.018363; current loss: 0.002948944412057925\n",
            "epoch 2/50 at time 12:56:46.119195; current loss: 0.002904636784904919\n",
            "epoch 3/50 at time 12:56:51.944218; current loss: 0.002863972377390452\n",
            "epoch 4/50 at time 12:56:59.255829; current loss: 0.002833395896619855\n",
            "epoch 5/50 at time 12:57:05.069530; current loss: 0.0028041937976609668\n",
            "epoch 6/50 at time 12:57:12.365060; current loss: 0.0027694544856208163\n",
            "epoch 7/50 at time 12:57:18.038568; current loss: 0.002741970080301734\n",
            "epoch 8/50 at time 12:57:25.043831; current loss: 0.0027180876553455804\n",
            "epoch 9/50 at time 12:57:30.960680; current loss: 0.0026962456078991072\n",
            "epoch 10/50 at time 12:57:37.716902; current loss: 0.002676496143568044\n",
            "epoch 11/50 at time 12:57:44.278092; current loss: 0.0026570286640081294\n",
            "epoch 12/50 at time 12:57:50.450236; current loss: 0.002636798548734244\n",
            "epoch 13/50 at time 12:57:57.449242; current loss: 0.0026225277062549694\n",
            "epoch 14/50 at time 12:58:03.247593; current loss: 0.0026088833609066277\n",
            "epoch 15/50 at time 12:58:10.467281; current loss: 0.0025911241623960465\n",
            "epoch 16/50 at time 12:58:16.199506; current loss: 0.002574401456926325\n",
            "epoch 17/50 at time 12:58:23.543623; current loss: 0.0025616563240406557\n",
            "epoch 18/50 at time 12:58:29.481089; current loss: 0.0025483101543651377\n",
            "epoch 19/50 at time 12:58:36.574325; current loss: 0.0025356154694382957\n",
            "epoch 20/50 at time 12:58:42.506679; current loss: 0.002523703777789067\n",
            "epoch 21/50 at time 12:58:48.927265; current loss: 0.00251169954354563\n",
            "epoch 22/50 at time 12:58:55.605544; current loss: 0.0025036272143498044\n",
            "epoch 23/50 at time 12:59:02.162984; current loss: 0.0024931393353393085\n",
            "epoch 24/50 at time 12:59:10.389420; current loss: 0.002483749539417603\n",
            "epoch 25/50 at time 12:59:16.146893; current loss: 0.0024748657922491267\n",
            "epoch 26/50 at time 12:59:23.557498; current loss: 0.0024668958899542823\n",
            "epoch 27/50 at time 12:59:29.264601; current loss: 0.00245946250691443\n",
            "epoch 28/50 at time 12:59:36.324616; current loss: 0.0024519730881267937\n",
            "epoch 29/50 at time 12:59:42.623484; current loss: 0.0024449879209447415\n",
            "epoch 30/50 at time 12:59:48.842566; current loss: 0.0024377601884027576\n",
            "epoch 31/50 at time 12:59:55.574915; current loss: 0.0024300243082258674\n",
            "epoch 32/50 at time 13:00:01.420175; current loss: 0.002423044924771741\n",
            "epoch 33/50 at time 13:00:08.687993; current loss: 0.0024157635045124687\n",
            "epoch 34/50 at time 13:00:14.541936; current loss: 0.0024110867777542336\n",
            "epoch 35/50 at time 13:00:21.830441; current loss: 0.0024028994482435733\n",
            "epoch 36/50 at time 13:00:27.582311; current loss: 0.0023955779238111563\n",
            "epoch 37/50 at time 13:00:34.897143; current loss: 0.002389879575230086\n",
            "epoch 38/50 at time 13:00:40.822446; current loss: 0.0023830443394318598\n",
            "epoch 39/50 at time 13:00:47.455476; current loss: 0.002376633857899576\n",
            "epoch 40/50 at time 13:00:53.973408; current loss: 0.002371628783661859\n",
            "epoch 41/50 at time 13:01:00.114325; current loss: 0.0023639335364059105\n",
            "epoch 42/50 at time 13:01:07.140509; current loss: 0.002357669798456515\n",
            "epoch 43/50 at time 13:01:12.833762; current loss: 0.0023511508196315846\n",
            "epoch 44/50 at time 13:01:20.142811; current loss: 0.0023468886060682608\n",
            "epoch 45/50 at time 13:01:26.076769; current loss: 0.0023394310026510225\n",
            "epoch 46/50 at time 13:01:33.644497; current loss: 0.0023332784540255882\n",
            "epoch 47/50 at time 13:01:39.387024; current loss: 0.002327751403789267\n",
            "epoch 48/50 at time 13:01:47.119888; current loss: 0.0023199342380313694\n",
            "epoch 49/50 at time 13:01:53.254928; current loss: 0.0023145113255457488\n",
            "epoch 50/50 at time 13:02:00.336237; current loss: 0.002310134645138477\n",
            "end of training 50 epochs: 2024-06-07 13:02:00.339535; elapsed time: 0:05:27.406482\n",
            "starting encoding at: 2024-06-07 13:02:00.347738\n",
            "starting decoding at: 2024-06-07 13:02:01.691224\n",
            "finish decoding at: 2024-06-07 13:02:02.568984\n",
            "MSE: 171.13829547763598\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    50.000000\n",
              "mean      0.002518\n",
              "std       0.000170\n",
              "min       0.002310\n",
              "25%       0.002385\n",
              "50%       0.002471\n",
              "75%       0.002619\n",
              "max       0.002949\n",
              "dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 156
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RRTflod5n0u4"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "vss-WbLAYObS",
        "nWjVMP_BZ7vW",
        "gs7Bdey6Yco8"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}